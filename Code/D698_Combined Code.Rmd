---
title: "D698 Final Project"
author: "Coffy Andrews-Guo, Vyanna Hill"
date: "`r Sys.Date()`"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Vignette Title}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r}
#Packages used
library(tidyverse)
library(MLmetrics)
library(ggpubr)
library(caret)
library(pROC) 
library(caTools)
library(h2o)

```


```{r eval=FALSE, warning=FALSE, include=FALSE}
#Pre-Processing
library(tidyverse)
library(mice)

#Uploading data sets for combination
elevation<-read_csv("C:/Users/walki/Documents/GitHub/D698/Datasets/USGS_CACounties_elevation_2023 (1).csv")
slope<-read_csv("C:/Users/walki/Documents/GitHub/D698/Datasets/SlopePercentage_Calitracts_LF2020.csv")
whp<-read_csv("C:/Users/walki/Documents/GitHub/D698/Datasets/WHP2020_ZipCode_Summary - zipcode_summary.csv")

weather<-read_csv("C:/Users/walki/Documents/GitHub/D698/Datasets/NOAA_CACounties_AverageTemp_2022.csv")
rainfall<-read_csv("C:/Users/walki/Documents/GitHub/D698/Datasets/NOAA_CACounties_AveragePercipitation_2022.csv")

LF_Vegdictonary<-read_csv("C:/Users/walki/Documents/GitHub/D698/Datasets/LF22_EVT_230 - LF22_EVT_230.csv.csv")
cali_vegtype<-read_csv("C:/Users/walki/Documents/GitHub/D698/Datasets/CalifornianTracts_VegType_2022LF - test.csv.csv")
#combining the weather data set first as both on the county lvl. Only want the averages of the year
cali_cweather<-weather%>%left_join(rainfall,by=join_by(ID))
cali_cweather<-cali_cweather%>%select(-c("Rank.x","Anomaly (1901-2000 base period).x","1901-2000 Mean.x","Name.y","State.y","Rank.y","Anomaly (1901-2000 base period).y","1901-2000 Mean.y"))
cali_cweather<-cali_cweather%>%rename(county_Id=ID,county_name=Name.x,State=State.x,avg_tempeture=Value.x,avg_precipitation=Value.y)

#onto topographic data, will need to combine by county and lat/long(if possible)
#Need the county ID in Slope data for left combine with elevation
cali_topography<-slope%>%unite("countyID",1:2,remove = FALSE,sep = "")

#14 rows missing slope data, can be zero slope but using mice for imputation
cali_topography<-cali_topography%>%rename(tract_avgSlope="_mean",tract_countSlope="_count",tract_maxSlope="_max")
cali_topography<-complete(mice(cali_topography,method = "cart",seed = 333))

elevation<-elevation%>%rename(countyID="County FIPS Code")

#mapping the tract data in topography by county ID and the closest match by Longitude
cali_topography<-cali_topography%>%inner_join(elevation,by=join_by(countyID,closest(INTPTLON<=Longitude)))

#removing unnecessary metrics and renaming columns for readability
cali_topography<-cali_topography%>%select(-c("Latitude","Longitude","Bgn Decision Date","Entry Date","Census Code","Census Classification Code","GSA Code","OPM Code","State FIPS Code","Map","State","Class","tract_maxSlope","Gaz ID","Feature Name","tract_countSlope","MTFCC","FUNCSTAT"))
cali_topography<-cali_topography%>%rename(tractID=NAME,land_Area=ALAND,water_Area=AWATER,latitude=INTPTLAT,longitude=INTPTLON,county_avgElevation=Elevation)

#Using LandFire's vegetation type dictionary to map tract's average vegetation type
#Filtering for CA tracts only
cali_vegetation<-cali_vegtype%>%filter(STUSPS=="CA")
lf_small<-LF_Vegdictonary%>%select("VALUE","EVT_NAME","EVT_LF","EVT_CLASS")
cali_vegetation<-cali_vegetation%>%left_join(lf_small,by=join_by(closest("_mean">=VALUE)))

#Cleaning up new data set
cali_vegetation<-cali_vegetation%>%select(-c("STATEFP","COUNTYFP","TRACTCE","AFFGEOID","NAME","NAMELSAD","STUSPS","NAMELSADCO","STATE_NAME","LSAD","ALAND","AWATER","_count","_sum","_mean","VALUE","EVT_NAME","EVT_CLASS"))


#Combing weather, topography, and vegetation
cali_features<-cali_topography%>%left_join(cali_cweather,by=join_by(County==county_name))
cali_features<-cali_features%>%select(-c("county_Id","State"))
cali_vegetation<-cali_vegetation%>% mutate(GEOID = paste("0", GEOID, sep = ""))

#Census Tract 9901 does not have vegetation as it is the shoreline, replacing NAs with Water
cali_features<-cali_features%>%left_join(cali_vegetation,by=join_by(GEOID))
cali_features<-cali_features%>%mutate(EVT_LF=replace_na(EVT_LF,"Water"))

#saving export
write.csv(cali_features,"caliTracts_features.csv")


#Reducing predictors from NRI before the combination

nri_data<-read_csv("C:/Users/walki/Documents/GitHub/D698/Datasets/NRI_Table_CensusTracts_Subset.csv")
nri<-nri_data%>%select("STATE","STATEABBRV","STATEFIPS","COUNTY","COUNTYTYPE","COUNTYFIPS","STCOFIPS","TRACT","TRACTFIPS","POPULATION","AREA","DRGT_EVNTS","DRGT_AFREQ","DRGT_HLRA","HWAV_EVNTS","HWAV_AFREQ","HWAV_HLRA","LTNG_EVNTS","LTNG_AFREQ","SWND_EVNTS","SWND_AFREQ","SWND_HLRA","WFIR_EVNTS","WFIR_AFREQ","WFIR_EXPA","WFIR_EXPT","WFIR_EXP_AREA","WFIR_HLRB","WFIR_HLRP","WFIR_HLRA","WFIR_HLRR","WFIR_EALT","WFIR_EALS","WFIR_EALR","WFIR_ALRA","WFIR_RISKV","WFIR_RISKS","WFIR_RISKR")

#only CA cases, converting categorical to binary
nri<-nri%>%filter(STATEABBRV=="CA")
nri<-nri%>%mutate(WFRI_R=case_when(WFIR_RISKV<13000~0,WFIR_RISKV>13000~1))

#Removal of extra columns
nri<-nri%>%select(-c(WFIR_EVNTS,WFIR_HLRR,WFIR_EALR))


#combination of cali features
cali_features<-read_csv("C:/Users/walki/Documents/GitHub/D698/Datasets/caliTracts_features.csv")

nri_cali<-nri%>%inner_join(cali_features,by=join_by(TRACTFIPS==GEOID))
nri_cali<-nri_cali%>%select(-c(STATE,STATEABBRV,STATEFIPS,COUNTY,COUNTYTYPE,COUNTYFIPS,STCOFIPS,TRACT,...1,STATEFP,COUNTYFP,TRACTCE,tractID,NAMELSAD,land_Area,latitude,longitude,County,countyID))

#renaming columns for reference
nri_cali<-nri_cali%>%rename(TRCT_WAREA=water_Area,TRCT_SLOPE=tract_avgSlope,CNTY_ELEV=county_avgElevation,CNTY_TEMP=avg_tempeture,CNTY_PRECIP=avg_precipitation,TRCT_VEGLF=EVT_LF)

write.csv(nri_cali,"nri_cali.csv")

```

```{r}

###Data Exploration
##using nri.cali Dataset
cali_data<-read_csv("C:/Users/walki/Documents/GitHub/D698/nri_cali.csv")

#Removing non important columns
#Note: WFIR_RISKS AND WFIR_RISV holds actual percentages of the likelihood, cannot be included as predictor value 
c_data<-cali_data%>%select(-c("...1","WFIR_RISKV","WFIR_RISKS","WFIR_HLRB"))

#Looking at the summary of all variables in the data set
summary(c_data)

#checking for null values
colSums(is.na(c_data))

#Reviewing the current distribution of the predictor values. See if there's future transformations
g1<-c_data%>%ggplot(aes(x=POPULATION))+geom_histogram(bins=20)+theme_light()
g2<-c_data%>%ggplot(aes(x=AREA))+geom_histogram(bins=20)+theme_light()
g3<-c_data%>%ggplot(aes(x=DRGT_EVNTS))+geom_histogram(bins=20)+theme_light()
g4<-c_data%>%ggplot(aes(x=DRGT_AFREQ))+geom_histogram(bins=20)+theme_light()
g7<-c_data%>%ggplot(aes(x=HWAV_AFREQ))+geom_histogram(bins=20)+theme_light()
g10<-c_data%>%ggplot(aes(x=LTNG_AFREQ))+geom_histogram(bins=20)+theme_light()
g14<-c_data%>%ggplot(aes(x=WFIR_AFREQ))+geom_histogram(bins=20)+theme_light()
g22<-c_data%>%ggplot(aes(x=WFIR_ALRA))+geom_histogram(bins=20)+theme_light()

#Plot for project write up, only a selected few
plt1<-ggarrange(g1,g2,g3,g4,g14,g22,g10,g7,nrow =4,ncol =2,align="h",heights = 2,font.label = list(size =3, color = "black"))

annotate_figure(plt1,top = text_grob("Distribution of Selected WildFire Predictor variables ",size=9))

#Reviewing a few variables on its boxplots in reflection with the response variable WFRI_R
g1<-c_data%>%ggplot(aes(y=TRCT_SLOPE,x=factor(WFRI_R)))+geom_boxplot()+theme_light()+labs(x="WildFire Present",y="Tract Slope")
g2<-c_data%>%ggplot(aes(y=WFIR_HLRA,x=factor(WFRI_R)))+geom_boxplot()+theme_light()+labs(x="WildFire Present",y="WildFire Histortic Argiculture Loss Ratio")
g3<-c_data%>%ggplot(aes(y=CNTY_PRECIP,x=factor(WFRI_R)))+geom_boxplot()+theme_light()+labs(x="WildFire Present",y="Annual County Precipitation")
g4<-c_data%>%ggplot(aes(y=LTNG_EVNTS,x=factor(WFRI_R)))+geom_boxplot()+theme_light()+labs(x="WildFire Present",y="Annual Lighting Events")

plt1<-ggarrange(g1,g2,g3,g4,nrow =2,ncol =2,align="h",heights = 2,font.label = list(size =3, color = "black"))

annotate_figure(plt1,top = text_grob("Difference in Summary Statistics With Wildfire presence",size=9))

#Checking if the data set is imbalanced
c_data%>%ggplot(aes(fill=WFRI_R))+geom_bar(aes(x=WFRI_R))+labs(title="WildFire Cases in the Data Set",x="WildFire Presence")


###Data Preparation

#transforming the tract vegetation life into binary dummy variables via mutate
unique(c_data$TRCT_VEGLF)
c_data<-c_data%>%mutate(.isShrub=if_else(TRCT_VEGLF=="Shrub",1,0),
                        .isTree=if_else(TRCT_VEGLF=="Tree",1,0),
                        .isDeveloped=if_else(TRCT_VEGLF=="Developed",1,0),
                        .isHerb=if_else(TRCT_VEGLF=="Herb",1,0),
                        .isArgiculture=if_else(TRCT_VEGLF=="Argiculture",1,0),
                        .isSparse=if_else(TRCT_VEGLF=="Sparse",1,0),
                        .isBarren=if_else(TRCT_VEGLF=="Barren",1,0),
                        .isSnowIce=if_else(TRCT_VEGLF=="Snow-Ice",1,0),
                        )
c_data<-c_data%>%select(-c("TRCT_VEGLF"))


#Seeing if there's multi-collinearity in the current predictors
#Drought events and drought frequency are highly correlated, Let's see with variable selection if one of the variables is dropped from the optimized predictor set
temp<-c_data%>%select(-c("WFRI_R","TRACTFIPS"))
temp<-cor(temp)

#setting the binary response and a few predictor variables as a factor before modeling
c_data<-c_data%>%mutate_at(c('WFRI_R',".isShrub",".isTree",".isDeveloped",".isHerb",".isArgiculture",".isSparse",".isBarren",".isSnowIce"),as.factor)

#Downsampling the non wildfire cases so the models can predict more fire cases
dwn_data<-downSample(x=c_data[,-ncol(c_data)],y=c_data$WFRI_R)

#Seeing new distribution
dwn_data%>%ggplot(aes(fill=WFRI_R))+geom_bar(aes(x=WFRI_R))+labs(title="WildFire Cases in the Data Set",x="WildFire Presence")

#splitting data set into testing and training
temp<-sample.split(dwn_data$WFRI_R,SplitRatio = 0.7)
training_data<-subset(dwn_data,temp==TRUE)
test_data<-subset(dwn_data,temp==FALSE)


### Data Analysis

#connection to h20 server
h2o.init()

#Uploading the data set into h2o and splitting the data set into training/test. Choosing a 70/30 split and splitting testing for a validation test
train.h2o<-as.h2o(training_data)
test.h2o<-as.h2o(test_data)

#Model 1| Random Forest

#removed wildfire exposure features it's possibly tied to the  likelihood response(,"WFIR_EXPA","WFIR_EXPT","WFIR_EXP_AREA")
features<-c("POPULATION","AREA","DRGT_AFREQ","DRGT_HLRA","HWAV_EVNTS","HWAV_AFREQ","HWAV_HLRA","LTNG_EVNTS","LTNG_AFREQ","SWND_EVNTS","SWND_AFREQ","SWND_HLRA","WFIR_AFREQ","WFIR_HLRP","WFIR_HLRA","TRCT_WAREA","TRCT_SLOPE","CNTY_ELEV","CNTY_TEMP","CNTY_PRECIP",".isShrub",".isTree",".isDeveloped",".isHerb",".isSparse",".isBarren")
response<-c("WFRI_R")

#Version 1 of Random Forest
#V1: Stopping metrics based on AUC score as preventing for overfitting and added Cross-validation with Nfolds
rf.model<-h2o.randomForest(x = features, y =response , training_frame = train.h2o, stopping_rounds = 5,stopping_tolerance = 0.001, stopping_metric = "AUC", seed = 3, balance_classes = FALSE, nfolds = 5,score_tree_interval=10)

#see the first version of the tree structure
rf.model@model$model_summary

#look at cross-validation results from the model, Not much difference between cases
rf.cross<-rf.model@model$cross_validation_metrics_summary%>%select(-c(mean,sd))

#review the feature importance of this random forest model and using the highest gini indexes in the final model
rf.features<-h2o.varimp(rf.model)
features_v2<-rf.features$variable[1:10]%>%as.vector()


#Version 2| RF
#Shortening max depth and trees for more conservative AUC. Applying highest gini index features to the model
rf.v2<-h2o.randomForest(x = features_v2, y =response , training_frame = train.h2o, stopping_rounds = 5,stopping_tolerance = 0.01, stopping_metric = "AUC", seed = 3, balance_classes = FALSE, nfolds = 10,score_tree_interval=10,max_depth=5,ntrees=25,min_rows = 20)


#plotting logloss of the revised model
plot(rf.v2)

#from the revised model, pull results from prediction against test set
rf.pred<-h2o.predict(rf.v2,test.h2o)%>%as.data.frame()%>%pull(predict)
rf.precsnprob<-h2o.predict(rf.v2,test.h2o)%>%as.data.frame()%>%pull(p1)
rf.reclprob<-h2o.predict(rf.v2,test.h2o)%>%as.data.frame()%>%pull(p0)

#confusion matrix of rf predictions
rf.con<-confusionMatrix(rf.pred,test_data$WFRI_R,positive = "1",mode = "prec_recall")

#storing confusion matrix results
confusionM<-rf.con$byClass%>%as.data.frame()%>%t()
confusionM<-as.data.frame(confusionM)
confusionM<-confusionM%>%rename(Pos_pred_value="Pos Pred Value",Neg_Pred_value="Neg Pred Value",Detection_rate="Detection Rate", Detection_prevalence="Detection Prevalence",Balanced_Accuracy="Balanced Accuracy")

#creating a reference table w/ predicted probabilities, actual, and predictions all in one
#see summary results of random forest model on the test data
rf.summary<-data.frame(
      obs<-test_data$WFRI_R,
      pred<-rf.pred,
      N<-rf.reclprob,
      Y<-rf.precsnprob
  )

rf.summary<-rf.summary%>%rename(obs="obs....test_data.WFRI_R",pred="pred....rf.pred", N="N....rf.reclprob", Y="Y....rf.precsnprob")

rf.auc<-roc(rf.summary$obs,Y)

test.results<-data.frame(
  t.R2<-R2_Score(y_pred = as.numeric(as.character(rf.summary$pred)),y_true =as.numeric(as.character( rf.summary$obs))),
  t.mse<-MSE(as.numeric(as.character(rf.summary$pred)),as.numeric(as.character(rf.summary$obs))),
  t.RSME<-RMSE(as.numeric(as.character(rf.summary$pred)),as.numeric(as.character(rf.summary$obs))),
  t.AUC<-rf.auc$auc,
  t.ClassError<-mean(rf.summary$pred!=rf.summary$obs)
  
)

#Stores Test Performance of Models
test.results<-test.results%>%rename(R2="t.R2....R2_Score.y_pred...as.numeric.as.character.rf.summary.pred....",MSE="t.mse....MSE.as.numeric.as.character.rf.summary.pred....as.numeric.as.character.rf.summary.obs...",RMSE="t.RSME....RMSE.as.numeric.as.character.rf.summary.pred....as.numeric.as.character.rf.summary.obs...",AUC="t.AUC....rf.auc.auc",classError="t.ClassError....mean.rf.summary.pred....rf.summary.obs.")

#plotting ROC curve of Random Forest
ggroc(rf.auc)+ggtitle("Random Forest ROC Curve of AUC= 0.9891")+geom_segment(aes(x=1,y=0,xend=0,yend=1),linetype="dotted",color="red")+theme_light()

#Model 2| Gradient Boosting Decision Tree

#Version 1| 
#Higher learning rate, Stopping metric on AUC as logloss saw lower R^2, Cross-validation on training set
gb_model<-h2o.gbm(x=features,y=response,training_frame = train.h2o,learn_rate = 0.1,ntrees=1000,stopping_rounds = 3,stopping_tolerance = 0.001,stopping_metric = "auc", score_tree_interval = 5,nfolds=5,seed=3)

#see the first version of the tree structure
gb_model@model$model_summary
gb.cross<-gb_model@model$cross_validation_metrics_summary%>%select(-c(mean,sd))

#Feature importance for gb_model
gb.features<-h2o.varimp(gb_model)
features_v2<-gb.features$variable[1:10]%>%as.vector()                                                                   
#Version 2
#Lowering stopping rounds for a quicker review on the AUC score, Smaller tree size from v1
gb_v2<-h2o.gbm(x=features_v2,y=response,training_frame = train.h2o,learn_rate = 0.1,ntrees=45,stopping_rounds = 2,stopping_tolerance = 0.001,stopping_metric = "auc", score_tree_interval = 5,nfolds=5,seed=3)
               
#see performance of prediction
gb.pred<-h2o.predict(gb_v2,test.h2o)%>%as.data.frame()%>%pull(predict)
gb.precsnprob<-h2o.predict(gb_v2,test.h2o)%>%as.data.frame()%>%pull(p1)
gb.reclprob<-h2o.predict(gb_v2,test.h2o)%>%as.data.frame()%>%pull(p0)

#Print the confusion matrix
gb.con<-confusionMatrix(gb.pred,test_data$WFRI_R,positive = "1",mode = "prec_recall")

#see summary results of random forest model on the test data
gb.summary<-data.frame(
      obs<-test_data$WFRI_R,
      pred<-gb.pred,
      N<-gb.reclprob,
      Y<-gb.precsnprob
  )

gb.summary<-gb.summary%>%rename(obs="obs....test_data.WFRI_R",pred="pred....gb.pred", N="N....gb.reclprob", Y="Y....gb.precsnprob")

gb.auc<-roc(gb.summary$obs,Y)


temp<-data.frame(
  t.R2<-R2_Score(y_pred = as.numeric(as.character(gb.summary$pred)),y_true =as.numeric(as.character( gb.summary$obs))),
  t.mse<-MSE(as.numeric(as.character(gb.summary$pred)),as.numeric(as.character(gb.summary$obs))),
  t.RSME<-RMSE(as.numeric(as.character(gb.summary$pred)),as.numeric(as.character(gb.summary$obs))),
  t.AUC<-gb.auc$auc,
  t.ClassError<-mean(gb.summary$pred!=gb.summary$obs)
  
)

#plotting ROC curve of gradient boosted DT
ggroc(gb.auc)+ggtitle("Gradient Boosted Decision Tree ROC Curve of AUC= 0.9871")+geom_segment(aes(x=1,y=0,xend=0,yend=1),linetype="dotted",color="red")+theme_light()


temp<-temp%>%rename(R2="t.R2....R2_Score.y_pred...as.numeric.as.character.gb.summary.pred....",MSE="t.mse....MSE.as.numeric.as.character.gb.summary.pred....as.numeric.as.character.gb.summary.obs..."  
,RMSE="t.RSME....RMSE.as.numeric.as.character.gb.summary.pred....as.numeric.as.character.gb.summary.obs..."
,AUC="t.AUC....gb.auc.auc",classError="t.ClassError....mean.gb.summary.pred....gb.summary.obs.")

#adding gradient boosting to the results data frame
test.results[2,]<-c(R2=temp$R2,MSE=temp$MSE,RMSE=temp$RMSE,AUC=temp$AUC,classError=temp$classError)
rownames(test.results)<-c("Random Forest","Gradient Boosting")


#storing confusion matrix testing results
temp<-gb.con$byClass%>%as.data.frame()%>%t()
temp<-as.data.frame(temp)

confusionM<-confusionM%>%add_row(Sensitivity= temp$Sensitivity,Specificity=temp$Specificity, Pos_pred_value=temp$`Pos Pred Value`,Neg_Pred_value=temp$`Neg Pred Value`,Precision=temp$Precision, Recall=temp$Recall,F1=temp$F1 ,Prevalence= temp$Prevalence,Detection_rate=temp$`Detection Rate`, Detection_prevalence=temp$`Detection Prevalence`,Balanced_Accuracy=temp$`Balanced Accuracy`)

rownames(confusionM)<-c("Random Forest","Gradient Boosting")


##Part 1 of analysis RF VS GB

#Current review on the two models
confusionM

#see ROCs of gradient boosted DT and random forest
rocs<-list(Gradient_Boost=gb.auc,Random_Forest=rf.auc)
ggroc(rocs)+ggtitle("ROC Performance of current models")+geom_segment(aes(x=1,y=0,xend=0,yend=1),linetype="dotted",color="red")+theme_light()


#see Precision-Recall Plots of the two models
rf.perf<-h2o.performance(rf.v2,test.h2o)%>%h2o.metric()%>%as.data.frame()%>%select(c(recall,precision))
rf.perf$model<-"Random Forest"
gb.perf<-h2o.performance(gb_v2,test.h2o)%>%h2o.metric()%>%as.data.frame()%>%select(c(recall,precision))
gb.perf$model<-"Gradient Boosted DT"
combine_rpplots<-rbind(rf.perf,gb.perf)

ggplot(combine_rpplots,aes(recall,precision,group=model,color=model))+geom_line()+labs(title ="Precision-Recall AUC Curve",legend="current ML models")+theme_light()


#Plotting residuals vs fitted
rf.summary<-rf.summary%>%mutate(resid=as.numeric(obs)-as.numeric(pred))
gb.summary<-gb.summary%>%mutate(resid=as.numeric(obs)-as.numeric(pred))
g1<-gb.summary%>%ggplot(aes(pred,resid))+geom_point()+labs(title="GBDT| Residuals vs Predicted",y="Residuals",x="Predicted")+theme_light()
g2<-rf.summary%>%ggplot(aes(pred,resid))+geom_point()+labs(title="RF| Residuals vs Predicted",y="Residuals",x="Predicted")+theme_light()

plt2<-ggarrange(g1,g2,ncol = 2)
annotate_figure(plt2,top = text_grob("Residuals vs Predicted values across Models",size=9))


#feature analysis. Plotting top ten feature by its gini score
rf.features<-h2o.varimp(rf.v2)%>%as.data.frame()
gb.features<-h2o.varimp(gb_v2)%>%as.data.frame()


g1<-rf.features%>%ggplot(aes(y=variable,x=scaled_importance))+geom_bar(stat="identity")+theme_light()
g2<-gb.features%>%ggplot(aes(y=variable,x=scaled_importance))+geom_bar(stat="identity")+theme_light()

plt3<-ggarrange(g1,g2,ncol = 2)
annotate_figure(plt3,top = text_grob("Feauture Importance Across Models",size=9))


## Model 3 |Auto ML Models



#using a base random forest for feature select
vimodel <- h2o.randomForest(x = features, y = response, training_frame = train.h2o,ntrees = 100)

# Compute variable importance of random forest model and use top performing features
variable_importance <- h2o.varimp(vimodel)
variable_importance <- variable_importance[order(-variable_importance$relative_importance), ]
variable_importance_df <- as.data.frame(variable_importance)

top_25_importances <- head(variable_importance_df, 25)

# Create a new variable to hold a data set of the top 25 important variables or features
cali_importance <- top_25_importances%>%select(variable)%>%as.vector()


# Run AutoML for 5 base models, Using the top 25 features in the base random forest model
aml <- h2o.automl(x = cali_importance$variable, y = response,
                  training_frame = train.h2o,
                  max_models = 5,
                  seed = 1)

# View the AutoML Leaderboard
lb <- aml@leaderboard
head(lb, n = nrow(lb))  # Print all rows instead of default (6 rows)

aml@leader

#Generate predictions on a test set, you can make predictions directly on the `H2OAutoML` object 
pred <- h2o.predict(aml, test.h2o) 

# Get leaderboard with all possible columns
lb <- h2o.get_leaderboard(object = aml, extra_columns = "ALL")
head(lb)


## Part 2 Analysis| Comparison of Top LM model compared to other two models

#Retrieve Top performing model and save separately
#saving name: GBM_1_AutoML_1_20231119_133450
winining_aml<-aml@leader
winining_aml@model$model_summary



#Retrieving Top performing Auto model and save its prediction separately for performance
stack.pred<-h2o.predict(aml@leader,test.h2o)%>%as.data.frame()%>%pull(predict)
stack.precsnprob<-h2o.predict(aml@leader,test.h2o)%>%as.data.frame()%>%pull(p1)
stack.reclprob<-h2o.predict(aml@leader,test.h2o)%>%as.data.frame()%>%pull(p0)



#Print the confusion matrix
stack.con<-confusionMatrix(stack.pred,test_data$WFRI_R,positive = "1",mode = "prec_recall")

#see summary results of stacked ensemble model on the test data
stack.summary<-data.frame(
      obs<-test_data$WFRI_R,
      pred<-stack.pred,
      N<-stack.reclprob,
      Y<-stack.precsnprob
  )


stack.summary<-stack.summary%>%rename(obs="obs....test_data.WFRI_R",pred="pred....stack.pred", N="N....stack.reclprob", Y="Y....stack.precsnprob")

stack.auc<-roc(stack.summary$obs,Y)


temp<-data.frame(
  t.R2<-R2_Score(y_pred = as.numeric(as.character(stack.summary$pred)),y_true =as.numeric(as.character( stack.summary$obs))),
  t.mse<-MSE(as.numeric(as.character(stack.summary$pred)),as.numeric(as.character(stack.summary$obs))),
  t.RSME<-RMSE(as.numeric(as.character(stack.summary$pred)),as.numeric(as.character(stack.summary$obs))),
  t.AUC<-stack.auc$auc,
  t.ClassError<-mean(stack.summary$pred!=stack.summary$obs)
  
)

#plotting ROC curve of gradient boosted DT
ggroc(stack.auc)+ggtitle("Stack Ensemble Model's ROC Curve of AUC= 0.9934")+geom_segment(aes(x=1,y=0,xend=0,yend=1),linetype="dotted",color="red")+theme_light()


temp<-temp%>%rename(R2="t.R2....R2_Score.y_pred...as.numeric.as.character.stack.summary.pred....",MSE="t.mse....MSE.as.numeric.as.character.stack.summary.pred....as.numeric.as.character.stack.summary.obs..."  
,RMSE="t.RSME....RMSE.as.numeric.as.character.stack.summary.pred...."
,AUC="t.AUC....stack.auc.auc",classError="t.ClassError....mean.stack.summary.pred....stack.summary.obs.")

#adding stacked ensemble to the results data frame
test.results[3,]<-c(R2=temp$R2,MSE=temp$MSE,RMSE=temp$RMSE,AUC=temp$AUC,classError=temp$classError)
rownames(test.results)<-c("Random Forest","Gradient Boosting","Stacked Ensemble")

#storing confusion matrix testing results
temp<-stack.con$byClass%>%as.data.frame()%>%t()
temp<-as.data.frame(temp)

confusionM<-confusionM%>%add_row(Sensitivity= temp$Sensitivity,Specificity=temp$Specificity, Pos_pred_value=temp$`Pos Pred Value`,Neg_Pred_value=temp$`Neg Pred Value`,Precision=temp$Precision, Recall=temp$Recall,F1=temp$F1 ,Prevalence= temp$Prevalence,Detection_rate=temp$`Detection Rate`, Detection_prevalence=temp$`Detection Prevalence`,Balanced_Accuracy=temp$`Balanced Accuracy`)

rownames(confusionM)<-c("Random Forest","Gradient Boosting","Stacked Ensemble")

#See precision and recall for all three models
stack.perf<-h2o.performance(winining_aml,test.h2o)%>%h2o.metric()%>%as.data.frame()%>%select(c(recall,precision))
stack.perf$model<-"Stacked Ensemble"
combine_rpplots<-rbind(rf.perf,gb.perf,stack.perf)

ggplot(combine_rpplots,aes(recall,precision,group=model,color=model))+geom_line()+labs(title ="Precision-Recall AUC Curve",legend="current ML models")+theme_light()

#see ROCs of all models
rocs<-list(Gradient_Boost=gb.auc,Random_Forest=rf.auc,Stacked_ensemble=stack.auc)
ggroc(rocs)+ggtitle("ROC Performance of all Models")+geom_segment(aes(x=1,y=0,xend=0,yend=1),linetype="dotted",color="red")+theme_light()



#h2o.shutdown()


````
